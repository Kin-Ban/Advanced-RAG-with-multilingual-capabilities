{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#import fitz  # PyMuPDF\n",
    "\n",
    "# Configuration Variables\n",
    "PINECONE_API_KEY = 'eb82ab1d-ccf9-473c-9262-b2418b5b4282'\n",
    "PINECONE_ENVIRONMENT = 'us-east-1'  # e.g., 'us-west1-gcp'\n",
    "#PINECONE_INDEX_NAME = 'multilingual-rag'\n",
    "HF_TOKEN = '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'\n",
    "EMBEDDING_MODEL = 'paraphrase-xlm-r-multilingual-v1'  # Or any suitable embedding model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_name=\"multilingual-rag\"\n",
    "pc = Pinecone(api_key='eb82ab1d-ccf9-473c-9262-b2418b5b4282')\n",
    "\n",
    "#create the index\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,#dimesion of dense vector\n",
    "        metric=\"dotproduct\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-multilingual-uncased. Creating a new one with mean pooling.\n",
      "e:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "), model_name='bert-base-multilingual-uncased', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"bert-base-multilingual-uncased\")\n",
    "embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x17a32cd4f20>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=pc.Index(index_name)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_scanned_pdf(pdf_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a PDF is scanned (image-based) or contains selectable text.\n",
    "    \"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text and text.strip():\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file. Uses OCR if the PDF is scanned.\n",
    "    \"\"\"\n",
    "    if is_scanned_pdf(pdf_path):\n",
    "        print(f\"Performing OCR on scanned PDF: {pdf_path}\")\n",
    "        images = convert_from_path(pdf_path)\n",
    "        text = \"\"\n",
    "        for img in images:\n",
    "            text += pytesseract.image_to_string(img)\n",
    "        return text\n",
    "    else:\n",
    "        print(f\"Extracting text from digital PDF: {pdf_path}\")\n",
    "        text = \"\"\n",
    "        # Use pdfplumber to extract text from a digital PDF\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "        return text\n",
    "def load_pdfs(pdf_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load all PDF file paths from a directory.\n",
    "    \"\"\"\n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(pdf_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "    return pdf_files\n",
    "\n",
    "def create_documents(pdf_paths: List[str]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Create a list of LangChain Document objects from PDF paths.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        documents.append(Document(page_content=text, metadata={\"source\": pdf_path}))\n",
    "    return documents\n",
    "\n",
    "def split_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks using RecursiveCharacterTextSplitter.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = []\n",
    "    for doc in documents:\n",
    "        split_docs.extend(splitter.split_documents([doc]))\n",
    "    return split_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer for 'bert-base-multilingual-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genrating sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_dict(input_batch):\n",
    "    # store a batch of sparse embeddings\n",
    "    sparse_emb = []\n",
    "    # iterate through input batch\n",
    "    for token_ids in input_batch:\n",
    "        # convert the input_ids list to a dictionary of key-to-frequency values\n",
    "        d = dict(Counter(token_ids))\n",
    "        \n",
    "        # filter out special tokens (101: CLS, 102: SEP, 103: MASK, 0: padding)\n",
    "        filtered_tokens = {key: d[key] for key in d if key not in [101, 102, 103, 0]}\n",
    "        \n",
    "        # separate indices and values and cast them to the expected types\n",
    "        indices = [int(key) for key in filtered_tokens.keys()]  # ensure indices are integers\n",
    "        values = [float(val) for val in filtered_tokens.values()]  # ensure values are floats\n",
    "        \n",
    "        # append the sparse vectors to sparse_emb list in the correct format\n",
    "        sparse_emb.append({'indices': indices, 'values': values})\n",
    "    \n",
    "    # return the sparse embeddings list\n",
    "    return sparse_emb\n",
    "\n",
    "def generate_sparse_vectors(context_batch):\n",
    "    # tokenize the input batch\n",
    "    inputs = tokenizer(\n",
    "        context_batch, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )['input_ids']\n",
    "    \n",
    "    # create sparse dictionaries in the format required by Pinecone\n",
    "    sparse_embeds = build_dict(inputs)\n",
    "    \n",
    "    return sparse_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating sparse + dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def upsert_to_pinecone(split_docs):\n",
    "    batch_size = 32\n",
    "\n",
    "    for i in tqdm(range(0, len(split_docs), batch_size)):\n",
    "        # find end of batch\n",
    "        i_end = min(i+batch_size, len(split_docs))\n",
    "        # extract batch\n",
    "        context_batch = split_docs[i:i_end]\n",
    "        \n",
    "        # extract the actual text from the Document objects\n",
    "        context_texts = [doc.page_content for doc in context_batch]\n",
    "        \n",
    "        # create unique IDs\n",
    "        ids = [str(x) for x in range(i, i_end)]\n",
    "        # add context passages as metadata\n",
    "        meta = [{'context': context_text} for context_text in context_texts]\n",
    "        \n",
    "        # create dense vectors (no need to call .tolist())\n",
    "        dense_embeds = embeddings.embed_documents(context_texts)\n",
    "        \n",
    "        # create sparse vectors\n",
    "        sparse_embeds = generate_sparse_vectors(context_texts)\n",
    "\n",
    "        vectors = []\n",
    "        # loop through the data and create dictionaries for uploading documents to pinecone index\n",
    "        for _id, sparse, dense, metadata in zip(ids, sparse_embeds, dense_embeds, meta):\n",
    "            vectors.append({\n",
    "                'id': _id,\n",
    "                'sparse_values': sparse,\n",
    "                'values': dense,\n",
    "                'metadata': metadata\n",
    "            })\n",
    "\n",
    "        # upload the documents to the new hybrid index\n",
    "        index.upsert(vectors=vectors)\n",
    "\n",
    "    # show index description after uploading the documents\n",
    "    index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now we can query the index, providing the sparse and \\ndense vectors of a question, along with a weight for keyword relevance (“alpha”). Alpha=1 will provide a purely semantic-based search result and \\nalpha=0 will provide a purely keyword-based result equivalent to BM25. The default value is 0.5.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Now we can query the index, providing the sparse and \n",
    "dense vectors of a question, along with a weight for keyword relevance (“alpha”). Alpha=1 will provide a purely semantic-based search result and \n",
    "alpha=0 will provide a purely keyword-based result equivalent to BM25. The default value is 0.5.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone_text.hybrid import hybrid_convex_scale\n",
    "\n",
    "def hybrid_query(question, top_k, alpha):\n",
    "    # convert the question into a sparse vector\n",
    "    sparse_vec = generate_sparse_vectors([question])\n",
    "    # convert the question into a dense vector\n",
    "    dense_vec = embeddings.embed_query([question]).tolist()\n",
    "    dense_vec, sparse_vector = hybrid_convex_scale(\n",
    "        dense_vec, sparse_vec, alpha=alpha\n",
    "    )\n",
    "    # query pinecone with the query parameters\n",
    "    result = index.query(\n",
    "        vector=dense_vec,\n",
    "        sparse_vector=sparse_vec[0],\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "      )\n",
    "    # return search results as json\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone_text.hybrid import hybrid_convex_scale\n",
    "\n",
    "def hybrid_query(question, top_k, alpha):\n",
    "    # Convert the question into a sparse vector\n",
    "    sparse_vec = generate_sparse_vectors([question])\n",
    "    \n",
    "    # Convert the question into a dense vector\n",
    "    dense_vec = embeddings.embed_query(question)  # No need for .tolist()\n",
    "\n",
    "    # Check the type and structure of dense_vec and sparse_vec\n",
    "    print(f'dense_vec: {dense_vec}, sparse_vec: {sparse_vec}')  # Debugging line\n",
    "\n",
    "    # Ensure the sparse vector is in the expected format\n",
    "    if isinstance(sparse_vec, list) and len(sparse_vec) > 0:\n",
    "        sparse_vector = sparse_vec[0]  # Adjust as needed based on expected input\n",
    "    else:\n",
    "        raise ValueError(\"sparse_vec must be a non-empty list of dictionaries.\")\n",
    "    \n",
    "    # Perform hybrid convex scaling\n",
    "    scaled_dense, scaled_sparse = hybrid_convex_scale(\n",
    "        dense_vec, sparse_vector, alpha=alpha\n",
    "    )\n",
    "    \n",
    "    # Query Pinecone with the query parameters\n",
    "    result = index.query(\n",
    "        vector=scaled_dense,\n",
    "        sparse_vector=scaled_sparse,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    \n",
    "    # Return search results as json\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files.\n"
     ]
    }
   ],
   "source": [
    "pdf_directory = r\"E:\\LLM projects\\Multilingual_RAG\\Advanced-RAG-with-multilingual-capabilities\\sample_pdfs\\en\\test_pdf\"  # Update this path\n",
    "pdf_paths = load_pdfs(pdf_directory)\n",
    "print(f\"Found {len(pdf_paths)} PDF files.\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from digital PDF: E:\\LLM projects\\Multilingual_RAG\\Advanced-RAG-with-multilingual-capabilities\\sample_pdfs\\en\\test_pdf\\The Alchemist by Paulo Coelho-1.pdf\n",
      "Extracted text and keywords from PDFs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = create_documents(pdf_paths)\n",
    "print(f\"Extracted text and keywords from PDFs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split documents into 261 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "split_docs = split_documents(documents)\n",
    "print(f\"Split documents into {len(split_docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [05:05<00:00, 33.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted all documents to Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Upsert documents to Pinecone\n",
    "upsert_to_pinecone(split_docs)\n",
    "print(f\"Upserted all documents to Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take user question\n",
    "user_question = \"What does Santiago notice in the sacristy of the abandoned church that he takes shelter in on the way to the merchant?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_vec: [-0.0874413475394249, 0.06685963273048401, -0.02957127057015896, 0.15570110082626343, -0.28925642371177673, 0.1513209342956543, 0.038793519139289856, 0.012834780849516392, -0.22768528759479523, 0.02760498784482479, -0.5841024518013, -0.15216900408267975, -0.1405993551015854, -0.19364595413208008, 0.3311733901500702, -0.007355878595262766, -0.013542771339416504, 0.14654631912708282, -0.24858324229717255, 0.13985984027385712, -0.08254257589578629, 0.16914409399032593, -0.22061055898666382, -0.02215919829905033, 0.5379860997200012, -0.23756997287273407, 0.0012062290916219354, 0.3764054775238037, -0.261172890663147, -0.09395284205675125, -0.08776575326919556, -0.06543240696191788, 0.06669039279222488, -0.09703323990106583, -0.16627539694309235, -0.2683883607387543, 0.028213748708367348, 0.5316920280456543, 0.06391129642724991, 0.1440470963716507, -0.0804431289434433, 0.32098588347435, -0.2043343186378479, 0.33363255858421326, -0.11518967151641846, -0.21422232687473297, 0.010109305381774902, 0.148982971906662, -0.20931053161621094, -0.28286227583885193, -0.12015373259782791, 0.050652068108320236, -0.39570364356040955, -0.12968303263187408, 0.0504380464553833, 0.009203478693962097, 0.03471815586090088, -4.7114986955421045e-05, 0.005509468261152506, -0.10476093739271164, 0.3882667124271393, -0.3393634855747223, -0.24870169162750244, 0.13684485852718353, 0.3388424813747406, -0.02593168057501316, 0.013699795119464397, 0.2851227819919586, -0.02461097575724125, 0.12015163898468018, 0.0032895654439926147, 0.0702597126364708, -0.0011812150478363037, -0.21814192831516266, -0.042361970990896225, -0.32657936215400696, 0.23555271327495575, -0.04137787967920303, -0.03898250684142113, 0.3018413782119751, 0.26135823130607605, -0.32102108001708984, -0.08308754116296768, 0.4418698847293854, -0.1377713829278946, 0.239344522356987, -0.2877519130706787, 0.4827648103237152, -0.04181842878460884, 0.17576830089092255, 0.2649877667427063, -0.08471611887216568, -0.3631618022918701, 0.07072417438030243, -0.007828235626220703, 0.2460344433784485, -0.5586631894111633, 0.5084965825080872, 0.03293273225426674, 0.4197206497192383, 0.3322628438472748, 0.10003325343132019, 0.13021506369113922, 0.00010328864300390705, 0.35153982043266296, -0.02355952002108097, 0.020190736278891563, -0.06705541163682938, 0.09120840579271317, 0.2002020627260208, 0.33407118916511536, 0.1038973405957222, -0.3071165978908539, -0.2660945951938629, -0.07031372934579849, 0.2983580529689789, -0.24040888249874115, -0.2256244421005249, 0.06081749126315117, -0.010558322072029114, -0.12192507833242416, 0.168784499168396, -0.30637040734291077, -0.5852503776550293, -0.17647719383239746, 0.039616864174604416, -0.21306473016738892, 0.4071316719055176, -0.17086023092269897, -0.08189034461975098, 0.20696575939655304, -0.07029625028371811, 0.5239488482475281, -0.41687795519828796, 0.12711554765701294, 0.09497489780187607, -0.10107815265655518, -0.020924994722008705, 0.08787713199853897, 0.22736452519893646, 0.3062037229537964, -0.4291207790374756, -0.03472684323787689, -0.12352975457906723, 0.11316034197807312, 0.3056061863899231, 0.0744069293141365, -0.3457176983356476, -0.22651131451129913, 0.3706282079219818, 0.023917624726891518, -0.015201658010482788, 0.12765033543109894, -0.10616762191057205, -0.17624901235103607, 0.37733975052833557, -0.37926483154296875, -0.07609603554010391, -0.3076476752758026, -0.9243507981300354, -0.014530976302921772, 0.23035436868667603, -0.4292120933532715, 0.05849214270710945, 0.45179983973503113, -0.13010291755199432, -0.2691144049167633, 0.16893607378005981, -0.2518323063850403, -0.018113553524017334, -0.09532514214515686, 0.04759157821536064, -0.13340647518634796, 0.11261608451604843, -0.024251312017440796, 0.09356940537691116, 0.1720932275056839, 0.15708093345165253, -0.23094214498996735, -0.04064120724797249, -0.07345504313707352, 0.2953837811946869, 0.13494877517223358, 0.06445629149675369, 0.03742623329162598, 0.4892179071903229, 0.02397209405899048, 0.012838169932365417, -0.022279441356658936, -0.14908476173877716, 0.21785064041614532, 0.11312248557806015, -0.05817720293998718, -0.0819973275065422, 0.10994073748588562, -0.3834090232849121, 0.03995541110634804, 0.3446083962917328, -0.0012398064136505127, 0.016987435519695282, -0.14868773519992828, 0.3533696234226227, -0.24602068960666656, 0.08468125015497208, -0.14920635521411896, -0.09297823905944824, -0.34890058636665344, 0.37675046920776367, -0.2788069546222687, 0.12231061607599258, 0.7723550200462341, -0.03166365996003151, -0.1299472451210022, -0.10159299522638321, -0.2789597809314728, 0.049576614052057266, -0.1765037626028061, -0.07898493856191635, 0.019404416903853416, 0.12436582893133163, -2.1437803297885694e-05, 0.13707663118839264, 0.3995244801044464, 0.08930936455726624, -0.42876553535461426, 0.16119134426116943, -0.01379345078021288, 0.2544706165790558, -0.13244815170764923, 0.006076600402593613, -0.06189365312457085, 0.13663901388645172, -0.1236676350235939, -0.007278269622474909, 0.29903873801231384, -0.16191677749156952, -0.08471109718084335, -0.08476105332374573, -0.04356636106967926, 0.15655657649040222, 0.026125101372599602, -0.23470287024974823, 0.10618860274553299, -0.040130436420440674, 0.45685815811157227, 0.34395328164100647, 0.2445705682039261, 0.19718988239765167, 0.45193958282470703, 0.2781989872455597, 0.0204872265458107, 0.27073046565055847, -0.5629972815513611, 0.1661377102136612, 0.28913071751594543, 0.08866026252508163, 0.18005229532718658, -0.1294538974761963, -0.2904707193374634, -0.12707196176052094, -0.5644508004188538, -0.3956986367702484, -0.22015340626239777, -0.07125043123960495, -0.15166328847408295, 0.034315187484025955, -0.12423083186149597, 0.17086942493915558, 0.022558322176337242, -0.5180451273918152, -0.22613759338855743, 0.01126360148191452, 0.29777470231056213, 0.1462128758430481, 0.08757614344358444, -0.15566615760326385, 0.20317672193050385, 0.1896064430475235, 0.025783216580748558, -0.09662701934576035, -0.6141902804374695, -0.32828518748283386, 0.014646793715655804, 0.5740578174591064, 0.2768271863460541, -0.11366365104913712, 0.2330571413040161, 0.10277247428894043, 0.15087734162807465, 0.36310136318206787, -1.092578411102295, 0.09345269948244095, 0.30427005887031555, -0.02220793068408966, -0.1011039987206459, 0.14002764225006104, -0.3357643187046051, -0.17654423415660858, -0.057088449597358704, 0.07677949219942093, 0.22602665424346924, -0.01420461107045412, -0.04125438258051872, 0.08730588108301163, -0.02186199091374874, 0.06853143125772476, -0.4107615053653717, 0.06502319872379303, -0.33486226201057434, -0.09904282540082932, -0.3085484802722931, 0.04129062965512276, 0.2507048547267914, 0.19436289370059967, -0.36628302931785583, 0.019547024741768837, -0.14029426872730255, -0.4224790334701538, -0.08110815286636353, -0.0490131676197052, -0.21720321476459503, -0.014771528542041779, -0.09247374534606934, -0.12690643966197968, -0.06705573946237564, 0.09751033037900925, 0.07874467223882675, -0.6539754271507263, 0.013008157722651958, 0.29761478304862976, 0.016119346022605896, 0.07721122354269028, -0.3018190562725067, -0.2971256673336029, -0.036231283098459244, -0.17730234563350677, 0.18991732597351074, -0.05338987335562706, -0.11597580462694168, -0.1494661271572113, 0.06840244680643082, -0.08270469307899475, -0.03300865367054939, -0.15967276692390442, 0.2967012822628021, -0.2882508635520935, 0.22550193965435028, 0.12574338912963867, -0.3687193691730499, 0.03888706490397453, -0.42068591713905334, -0.14297322928905487, -0.08672481030225754, -0.3103160560131073, 0.15735822916030884, -0.16432137787342072, 0.10464056581258774, 0.44884538650512695, 0.18659226596355438, 0.3802047669887543, -0.13071803748607635, 0.36925873160362244, -0.2123812437057495, -0.36093100905418396, 0.29372638463974, -0.6684129238128662, 0.4165305197238922, 0.3014972507953644, 0.1630427986383438, -0.18963973224163055, 0.08924923092126846, -0.2534865736961365, 0.10735932737588882, 0.32707563042640686, -0.14853410422801971, -0.09298472851514816, 0.10686389356851578, 0.38441911339759827, -0.0274150762706995, 0.08396229892969131, 0.146250918507576, 0.1720418930053711, -0.12922848761081696, 0.044760897755622864, -0.0317358635365963, -0.2719033360481262, -0.3044719398021698, 0.3122114837169647, -0.1648680418729782, 0.1834365874528885, 0.2748551070690155, 0.15292204916477203, -0.5122225284576416, 0.051597218960523605, 0.02015507221221924, -0.3795943558216095, 0.14630866050720215, 0.13710881769657135, 0.0974729061126709, 0.7855055332183838, -0.00435904273763299, 0.20571184158325195, -0.40140804648399353, 0.1923147588968277, -0.23626571893692017, 0.0991131067276001, 0.0041074552573263645, 0.6303877234458923, 0.11357223987579346, -0.777656614780426, -0.8394660949707031, 0.08426056057214737, 0.11839652061462402, -1.8775652647018433, 0.2680850327014923, 0.12640999257564545, 0.05327610298991203, -0.0991755947470665, -0.07289643585681915, 0.09217864274978638, 0.09637095779180527, -0.3451804220676422, 0.4195663034915924, -0.22973446547985077, 0.351106733083725, 0.17622508108615875, 0.07431752234697342, 0.17651556432247162, 0.6910495162010193, 0.18678070604801178, 0.024102186784148216, 0.03632659092545509, 0.1856164187192917, 0.06419882923364639, -0.2875267267227173, 0.12734107673168182, 0.28020042181015015, -0.27054494619369507, -0.27614250779151917, 0.018269645050168037, 0.27685779333114624, 0.1497972011566162, 0.20747338235378265, -0.2561965584754944, 0.03968647122383118, 0.11266762018203735, 0.2419932633638382, 0.9748525619506836, -0.254739910364151, 0.4554978609085083, -0.0057389140129089355, 0.12111572176218033, -0.15436683595180511, 0.18953068554401398, -0.11077449470758438, 0.23354220390319824, -0.47599875926971436, 0.10501643270254135, -0.05727272108197212, 0.5349690318107605, -0.17375773191452026, 0.04764243960380554, -0.014330203644931316, 0.39168763160705566, -0.08365658670663834, 0.24301773309707642, -0.2743973731994629, 0.05217363312840462, -0.021433642134070396, -0.269806832075119, 0.4150373041629791, 0.16980735957622528, -0.22720593214035034, -0.06242573261260986, 0.02234223484992981, 0.00482185697183013, 0.12514948844909668, -0.28092119097709656, 0.11207716912031174, 0.013241206295788288, -0.3203231394290924, -0.3832227289676666, 0.46632662415504456, 0.12721498310565948, 0.5547134876251221, -0.02260534279048443, 0.18585389852523804, 0.2727924883365631, 0.11781283468008041, 0.02758706547319889, -0.01996755599975586, -0.37914061546325684, 0.09268365055322647, 0.044830191880464554, -0.36746275424957275, -0.12007641792297363, 0.17176192998886108, -0.08507251739501953, 0.2952186167240143, 0.5964245796203613, 0.08952799439430237, 0.24154920876026154, -0.3950188457965851, -0.08990531414747238, -0.05416922643780708, 0.043244823813438416, 0.11590426415205002, -0.37999996542930603, -0.11360207945108414, 0.03812392055988312, 0.08622181415557861, 0.45794200897216797, -0.11376108974218369, 0.5967028141021729, -0.34638547897338867, -0.1828300505876541, 0.16925549507141113, -0.061068665236234665, 0.30528751015663147, -0.12565895915031433, 0.0993746742606163, -0.1351723074913025, 0.2233458310365677, -0.28463122248649597, -0.174488365650177, -0.09470120817422867, 0.07551782578229904, -0.275118350982666, -0.01046846341341734, 0.242070272564888, 0.1661939024925232, -0.2363201528787613, -0.03125995025038719, 0.12049907445907593, 0.32003486156463623, 0.41107621788978577, 0.039648089557886124, -0.29944565892219543, -0.05821197107434273, 0.07515215128660202, -0.3290034830570221, -0.5224032998085022, -0.26253923773765564, -0.02245093323290348, 0.20013980567455292, 0.18674199283123016, -0.5092362761497498, 0.15420864522457123, -0.10371091216802597, -0.2261456996202469, -0.17323505878448486, 0.3799072206020355, 0.10350147634744644, 0.20518343150615692, -0.25878027081489563, -0.039262447506189346, -0.16820158064365387, -0.03257985785603523, -0.4427781105041504, -0.2861120402812958, 0.1096920296549797, 0.20949123799800873, 0.038495492190122604, -0.7185012698173523, -0.1589362472295761, 0.39562979340553284, -0.36100825667381287, 0.08247257024049759, 0.02604024112224579, -0.35280799865722656, 0.49245771765708923, -0.2818026840686798, -0.2806289494037628, -0.16858841478824615, -0.21996520459651947, 0.15272803604602814, 0.027300583198666573, 0.07740849256515503, 0.12641490995883942, -0.09344065934419632, 0.3362318277359009, 0.07257231324911118, 0.16484160721302032, 0.22586949169635773, 0.23673538863658905, 0.2714967131614685, -0.271562784910202, -0.0470036081969738, -0.007554732263088226, -0.07384709268808365, -0.5727264285087585, 0.27004876732826233, 0.22395725548267365, -0.12704519927501678, -0.036624569445848465, 0.015403996221721172, 0.3539635241031647, -0.0354386605322361, 0.2754337787628174, -0.06546834111213684, -0.27730485796928406, -0.2930399179458618, 0.2996692955493927, -0.13168779015541077, -0.011189441196620464, 0.3935662806034088, 0.40021419525146484, -0.15669919550418854, -0.3763154447078705, -0.003507280023768544, -0.05210597813129425, -0.1317969411611557, 0.15686139464378357, 0.231479212641716, 0.18442441523075104, -0.18444734811782837, -0.021155042573809624, 0.0741765946149826, 0.1751520186662674, 0.38000449538230896, 0.20275050401687622, -0.1556883007287979, 0.07549174875020981, -0.132881760597229, 0.11374881118535995, -0.38790687918663025, -0.1888677328824997, 0.04312220215797424, 0.03659285977482796, -0.11343696713447571, -0.32029929757118225, 0.09577015787363052, -0.32599568367004395, 0.078419990837574, 0.2700153589248657, -0.6020002961158752, 0.48175907135009766, 0.43409979343414307, -0.25243088603019714, -0.2142411470413208, 0.3247028589248657, 0.5616686940193176, -0.016339967027306557, 0.08579670637845993, -0.026394998654723167, 0.0019872114062309265, 0.3851136267185211, 0.3440721333026886, -0.0654958188533783, 0.1382458359003067, -0.5709837079048157, 0.04845995828509331, -0.10222289711236954, -0.0021794710773974657, -0.04503403231501579, -0.24851934611797333, -0.19079411029815674, -0.09771903604269028, -0.04225761070847511, 0.48581206798553467, 0.09641235321760178, 0.7958641648292542, -0.3223617970943451, 0.012006002478301525, 0.1616654396057129, 0.07708750665187836, 0.1307152956724167, 0.07144981622695923, 0.09391654282808304, -0.0402827151119709, 0.06445378810167313, 0.09696727991104126, 0.31201374530792236, -0.06141994521021843, 0.02918074280023575, -0.05404259264469147, 0.37742701172828674, -0.09352663904428482, 0.06395471841096878, 0.021399864926934242, -0.33574917912483215, 0.32091009616851807, -0.05242456495761871, -0.21170108020305634, 0.03186339512467384, 0.07032844424247742, 0.027409620583057404, -0.1317347139120102, 0.09128845483064651, -0.26671290397644043, 0.3131580054759979, -0.06805265694856644, -0.07486537098884583, -0.36023056507110596, 0.19617849588394165, -0.29271450638771057, 0.157382071018219, 0.2499958723783493, 0.27181893587112427, 0.17856329679489136, 1.013970971107483, -0.2932029068470001, -0.013863440603017807, -0.16244171559810638, -0.14452610909938812, -0.0038059826474636793, -0.419063001871109, -0.09655999392271042, -0.037719208747148514, -0.23177595436573029, 0.25178518891334534, -0.20541620254516602, -0.10690806061029434, 0.04575398936867714, 0.03030773065984249, 0.049999792128801346, -0.14729462563991547, 0.16194091737270355, -0.019927537068724632, -0.03531176224350929, 0.2026841640472412, -0.03922151401638985, -0.040475454181432724, 0.16637031733989716, -0.18944783508777618, 0.15375003218650818, -0.19758240878582, 0.3256038427352905, -0.4316308796405792, 0.1099303662776947, 0.37112876772880554, 0.6157544255256653, -0.058746080845594406, 0.24927394092082977, 0.49180641770362854, 0.04967355355620384, 0.16980858147144318, 0.05283188819885254, 0.05894280597567558, -0.02491128444671631, 0.13636890053749084, -0.39929601550102234, -0.4771442711353302, -0.4278385639190674, 0.019248994067311287, -0.4535263478755951, 0.09017916768789291, 0.13648851215839386, -0.4506126940250397, 0.23134298622608185, -0.00762195372954011, -0.14661267399787903, 0.05274401232600212, -0.3215426504611969, -0.08690091222524643, -0.17703884840011597, 0.1832180768251419, 0.20368100702762604, 0.33525797724723816, 0.5576339364051819, -0.42007359862327576, 0.0410347543656826, -0.026330774649977684, -0.19753718376159668, 0.13273455202579498, 0.004162618424743414, 0.14870314300060272, -0.4381893575191498], sparse_vec: [{'indices': [10488, 10140, 12450, 136], 'values': [1.0, 1.0, 1.0, 1.0]}]\n",
      "{'matches': [{'id': '52',\n",
      "              'metadata': {'context': 'money in his pouch, and the boy knew\\n'\n",
      "                                      'that in money there was magic;\\n'\n",
      "                                      'whoever has money is never really\\n'\n",
      "                                      'alone. Before long, maybe in just a '\n",
      "                                      'few\\n'\n",
      "                                      'days, he would be at the Pyramids. An\\n'\n",
      "                                      'old man, with a breastplate of gold,\\n'\n",
      "                                      \"wouldn't have lied just to acquire six\\n\"\n",
      "                                      'sheep.The old man had spoken about '\n",
      "                                      'signs\\n'\n",
      "                                      'and omens, and, as the boy was\\n'\n",
      "                                      'crossing the strait, he had thought\\n'\n",
      "                                      'about omens. Yes, the old man had\\n'\n",
      "                                      'known what he was talking about:\\n'\n",
      "                                      'during the time the boy had spent in '\n",
      "                                      'the\\n'\n",
      "                                      'fields of Andalusia, he had become\\n'\n",
      "                                      'used to learning which path he should\\n'\n",
      "                                      'take by observing the ground and the\\n'\n",
      "                                      'sky. He had discovered that the\\n'\n",
      "                                      'presence of a certain bird meant that '\n",
      "                                      'a\\n'\n",
      "                                      'snake was nearby, and that a certain\\n'\n",
      "                                      'shrub was a sign that there was water '\n",
      "                                      'in\\n'\n",
      "                                      'the area. The sheep had taught him\\n'\n",
      "                                      'that.\\n'\n",
      "                                      'If God leads the sheep so well, he '\n",
      "                                      'will\\n'\n",
      "                                      'also lead a man, he thought, and '\n",
      "                                      'thatmade him feel better. The tea '\n",
      "                                      'seemed\\n'\n",
      "                                      'less bitter.\\n'\n",
      "                                      '\"Who are you?\" he heard a voice ask\\n'\n",
      "                                      'him in Spanish.\\n'\n",
      "                                      'The boy was relieved. He was thinking'},\n",
      "              'score': 14.9272184,\n",
      "              'values': []},\n",
      "             {'id': '254',\n",
      "              'metadata': {'context': 'moonlight, and the boy could see\\n'\n",
      "                                      'neither their eyes nor their faces.\\n'\n",
      "                                      '\"What are you doing here?\" one of the\\n'\n",
      "                                      'figures demanded.\\n'\n",
      "                                      'Because he was terrified, the boy '\n",
      "                                      \"didn'tanswer. He had found where his\\n\"\n",
      "                                      'treasure was, and was frightened at\\n'\n",
      "                                      'what might happen.\\n'\n",
      "                                      '\"We\\'re refugees from the tribal wars,\\n'\n",
      "                                      'and we need money,\" the other figure\\n'\n",
      "                                      'said. \"What are you hiding there?\"\\n'\n",
      "                                      '\"I\\'m not hiding anything,\" the boy\\n'\n",
      "                                      'answered.\\n'\n",
      "                                      'But one of them seized the boy and\\n'\n",
      "                                      'yanked him back out of the hole.\\n'\n",
      "                                      \"Another, who was searching the boy's\\n\"\n",
      "                                      'bags, found the piece of gold.\\n'\n",
      "                                      '\"There\\'s gold here,\" he said.\\n'\n",
      "                                      'The moon shone on the face of theArab '\n",
      "                                      'who had seized him, and in the\\n'\n",
      "                                      \"man's eyes the boy saw death.\\n\"\n",
      "                                      '\"He\\'s probably got more gold hidden '\n",
      "                                      'in\\n'\n",
      "                                      'the ground.\"\\n'\n",
      "                                      'They made the boy continue digging,\\n'\n",
      "                                      'but he found nothing. As the sun rose,\\n'\n",
      "                                      'the men began to beat the boy. He was\\n'\n",
      "                                      'bruised and bleeding, his clothing was\\n'\n",
      "                                      'torn to shreds, and he felt that death\\n'\n",
      "                                      'was near.\\n'\n",
      "                                      '\"What good is money to you if you\\'re'},\n",
      "              'score': 14.5191917,\n",
      "              'values': []},\n",
      "             {'id': '248',\n",
      "              'metadata': {'context': 'rabbi who was able to cure illnesses,\\n'\n",
      "                                      'and he rode out for days and days in\\n'\n",
      "                                      'search of this man. Along the way, he\\n'\n",
      "                                      'learned that the man he was seeking\\n'\n",
      "                                      'was the Son of God. He met others who\\n'\n",
      "                                      'had been cured by him, and they\\n'\n",
      "                                      \"instructed your son in the man's\\n\"\n",
      "                                      'teachings. And so, despite the fact '\n",
      "                                      'that\\n'\n",
      "                                      'he was a Roman centurion, he\\n'\n",
      "                                      'converted to their faith. Shortly\\n'\n",
      "                                      'thereafter, he reached the place where\\n'\n",
      "                                      'the man he was looking for was\\n'\n",
      "                                      'visiting.\\'\" \\'He told the man that one '\n",
      "                                      'of his\\n'\n",
      "                                      'servants was gravely ill, and the '\n",
      "                                      'rabbi\\n'\n",
      "                                      'made ready to go to his house with\\n'\n",
      "                                      'him. But the centurion was a man of\\n'\n",
      "                                      'faith, and, looking into the eyes of '\n",
      "                                      'the\\n'\n",
      "                                      'rabbi, he knew that he was surely in '\n",
      "                                      'the\\n'\n",
      "                                      \"presence of the Son of God.'\\n\"\n",
      "                                      '\" \\'And this is what your son said,\\' '\n",
      "                                      'the\\n'\n",
      "                                      \"angel told the man. 'These are the\\n\"\n",
      "                                      'words he said to the rabbi at that '\n",
      "                                      'point,\\n'\n",
      "                                      'and they have never been forgotten:\\n'\n",
      "                                      '\"My Lord, I am not worthy that you\\n'\n",
      "                                      'should come under my roof. But only\\n'\n",
      "                                      'speak a word and my servant will be\\n'\n",
      "                                      'healed.\" \"\\''},\n",
      "              'score': 14.5031719,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 11}}\n"
     ]
    }
   ],
   "source": [
    "# hybrid retriever\n",
    "retriever_output = hybrid_query(user_question,3,0.5)\n",
    "#print(\"Hybrid retriever initialized.\")\n",
    "print(retriever_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82a68b1b275400dbc7f6e6ab4508df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kinja\\.cache\\huggingface\\hub\\models--google--mt5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b42b735e5f4b449c924aa1e124fea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1027f0f9b0140788793d785cc977381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7b26ea164d44769b7c0e49ee4be5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "e:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "e:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:983: UserWarning: Not enough free disk space to download the file. The expected file size is: 4918.58 MB. The target location C:\\Users\\kinja\\.cache\\huggingface\\hub\\models--google--mt5-large\\blobs only has 155.67 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5162daddf70440d780d3e6f59e8059cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/mt5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# mT5-1.2B is equivalent to mt5-large\u001b[39;00m\n\u001b[0;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create a text generation pipeline\u001b[39;00m\n\u001b[0;32m     18\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m                \u001b[38;5;66;03m# Set to 0 if using GPU, -1 for CPU\u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n",
      "File \u001b[1;32me:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32me:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\transformers\\modeling_utils.py:3633\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3631\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[0;32m   3632\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[1;32m-> 3633\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3634\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[0;32m   3635\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[0;32m   3637\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[0;32m   3638\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3639\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3640\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[0;32m   3641\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[0;32m   3642\u001b[0m     )\n",
      "File \u001b[1;32me:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32me:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[0;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m   1213\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1229\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1230\u001b[0m     )\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1381\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1379\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1381\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1392\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32me:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1915\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32me:\\LLM projects\\Multilingual_RAG\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:544\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    543\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[1;32m--> 544\u001b[0m     \u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m     new_resume_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# Install necessary packages if not already installed\n",
    "# pip install transformers torch langchain sentencepiece langchain-huggingface\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Load the mT5-1.2B model and tokenizer\n",
    "model_name = \"google/mt5-large\"  # mT5-1.2B is equivalent to mt5-large\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,          # Max length for generated text\n",
    "    temperature=0.7,         # Adjust temperature for variability\n",
    "    top_p=0.9,               # Nucleus sampling\n",
    "    num_return_sequences=1,  # We only want one output\n",
    "    truncation=True,         # Enable truncation\n",
    "    device=-1                # Set to 0 if using GPU, -1 for CPU\n",
    ")\n",
    "\n",
    "# Wrap the pipeline with LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "def query_decompose(question):\n",
    "    # Define the refined prompt template for question decomposition\n",
    "    template = \"\"\"You are a helpful assistant that generates three distinct sub-questions related to the main question provided. \n",
    "    Each sub-question should focus on a specific aspect of the main question, and the sub-questions must be clear, concise, and relevant.\n",
    "\n",
    "    Main Question: {question}\n",
    "\n",
    "    Please list the sub-questions as a numbered list:\n",
    "    1.\"\"\"\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt_decomposition = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    # Function to parse output into a list of sub-questions\n",
    "    def parse_output(output):\n",
    "        return output[0]['generated_text'].split('\\n')\n",
    "\n",
    "    # Create RunnableSequence for generating sub-questions\n",
    "    generate_queries_decomposition = (\n",
    "        prompt_decomposition | llm | parse_output\n",
    "    )\n",
    "\n",
    "    # Example usage\n",
    "    if __name__ == \"__main__\":\n",
    "        try:\n",
    "            question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "            questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "            print(\"Generated Sub-Questions:\")\n",
    "            for idx, q in enumerate(questions, 1):\n",
    "                print(f\"{idx}. {q.strip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Path to the directory containing PDF files\n",
    "    pdf_directory = \"path_to_your_pdfs\"  # Update this path\n",
    "    \n",
    "    # Load PDF file paths\n",
    "    pdf_paths = load_pdfs(pdf_directory)\n",
    "    print(f\"Found {len(pdf_paths)} PDF files.\")\n",
    "    \n",
    "    # Create Document objects with keywords\n",
    "    documents = create_documents(pdf_paths)\n",
    "    print(f\"Extracted text and keywords from PDFs.\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    split_docs = split_documents(documents)\n",
    "    print(f\"Split documents into {len(split_docs)} chunks.\")\n",
    "    \n",
    "    # Upsert documents to Pinecone\n",
    "    upsert_to_pinecone(split_docs)\n",
    "    print(f\"Upserted all documents to Pinecone.\")\n",
    "\n",
    "    #Take user question\n",
    "    user_question = input(\"Enter your question: \")\n",
    "\n",
    "    #Query Decompose\n",
    "    sub_questions = query_decompose(user_question)\n",
    "    \n",
    "    # hybrid retriever\n",
    "    retriever_output = hybrid_query(sub_questions,2,0.5)\n",
    "    #print(\"Hybrid retriever initialized.\")\n",
    "    print(retriever_output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
